{
    "primary-gpt": {
        "class": "primary-gpt",
        "use_model_name": "gpt-4",
    },
    "fallback-gpt":{
        "class": "fallback-gpt",
        "use_model_name": "gpt-4o",
    },
    
    "gpt-4": {
        "class": "primary-gpt",
        "use_model_name": "gpt-4",
        "model_info_url": "https://platform.openai.com/docs/models/gpt-4",
        "modalities": {
            "input": [ "text", "image" ],
            "output": [ "text" ],
        },
        "capabilities": [ "tools" ],
        "context_window_size": 128000,
        "max_output_tokens": 16384,
        "knowledge_cutoff": "09/30/2023",
    },
    
    "gpt-4o": {
        "class": "fallback-gpt",
        "use_model_name": "gpt-4o",
        "model_info_url": "https://platform.openai.com/docs/models/gpt-4o",
        "modalities": {
            "input": [ "text", "image" ],
            "output": [ "text" ],
        },
        "capabilities": [ "tools" ],
        "context_window_size": 128000,
        "max_output_tokens": 16384,
        "knowledge_cutoff": "09/30/2023",
    },
    

    "classes": {
        "factories": [ "llm_config.llm_factory.LlmFactory" ],

        "primary-gpt": {
            "token_counting": "callback",   # Uses OpenAICallbackHandler
            "args": {
                "temperature": 0.1,
                "openai_api_key": null,
                "openai_proxy": null,
                "request_timeout": null,
                "max_retries": 2,
                "presence_penalty": null,
                "frequency_penalty": null,
                "seed": null,
                "logprobs": null,
                "top_logprobs": null,
                "logit_bias": null,
                "top_p": null,
                "max_tokens": null,         # This is always for output
                "tiktoken_model_name": null,
                "stop": null,

                # If you really need more parameters, you will likely have to create your own LlmFactory.
            }
        },
        "fallback-gpt": {
            "token_counting": "callback",   # Uses OpenAICallbackHandler
            "args": {
                "temperature": 0.1,
                "openai_api_key": null,
                "openai_api_base": null,
                "openai_organization": null,
                "openai_proxy": null,
                "request_timeout": null,
                "max_retries": 2,
                "presence_penalty": null,
                "frequency_penalty": null,
                "seed": null,
                "logprobs": null,
                "top_logprobs": null,
                "logit_bias": null,
                "top_p": null,
                "max_tokens": null,         # This is always for output
                "tiktoken_model_name": null,
                "stop": null,

                # If you really need more parameters, you will likely have to create your own LlmFactory.
            }
        },
    }
        "default_config":{

        "model_name": "gpt-4o",             # The string name of the default model to use.

        "temperature": 0.1,                 # The default LLM temperature (randomness) to use.
                                            # Values are floats between 0.0 (least random) to
                                            # 1.0 (most random).

        "prompt_token_fraction": 1.0,       # The fraction of total tokens (not necessarily words
                                            # or letters) to use for a prompt. Each model_name
                                            # has a documented number of max_tokens it can handle
                                            # which is a total count of message + response tokens
                                            # which goes into the calculation involved in
                                            # get_max_prompt_tokens().
                                            # By default the value is 1.0.

        "max_tokens": null,                 # The maximum number of tokens to use in
                                            # computing prompt tokens. By default this comes from
                                            # the model description in this class.

        # The following are more agent-level control as opposed to LLM control above.

        "verbose": false,                   # Default is boolean false for quiet server operation.
                                            # When True, responses from ChatEngine are logged to stdout.
                                            # Can be "extra" to engage even more verbose logging from agent level.

        "max_iterations": 50,               # Agent control for how many iterations are done.
                                            # People often want to increase this when there are timeouts
                                            # and other retry-able errors due to bad network weather,
                                            # but this is a decent default set by langchain.

        "max_execution_seconds": 120,       # Amount of time an agent should keep trying in retry
                                            # situations before giving up.
    },
        
}